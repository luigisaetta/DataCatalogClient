{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technical-partnership",
   "metadata": {},
   "source": [
    "21/3/2021 Modified adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distant-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import pytorch_tabnet\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import ads\n",
    "import oci\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "\n",
    "from utils import get_hash_from_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conceptual-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting security: using Resource Principal\n",
    "ads.set_auth(auth='resource_principal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabulous-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File hash from Data Catalog is: credit_scoring/tabnet2_optuna\n"
     ]
    }
   ],
   "source": [
    "# getting information from Data Catalog\n",
    "FILE_NAME = \"cs-test.csv\"\n",
    "\n",
    "# md5 read from Catalog\n",
    "md5_cat = get_hash_from_catalog(FILE_NAME)\n",
    "\n",
    "print('File hash from Data Catalog is:', md5_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset from object storage, check MD5 hash\n",
    "BUCKET_NAME = \"credit_scoring\"\n",
    "TMP_FILE = 'temp.csv'\n",
    "\n",
    "ds = DatasetFactory.open(f\"ocis://{BUCKET_NAME}/{FILE_NAME}\")\n",
    "\n",
    "print('The dataset contains:', ds.shape[0], 'records')\n",
    "\n",
    "# dump to a tmp file\n",
    "ds.to_csv('temp.csv', index=None)\n",
    "\n",
    "md5_computed = hashlib.md5(open(TMP_FILE,'rb').read()).hexdigest()\n",
    "\n",
    "os.remove(TMP_FILE)\n",
    "\n",
    "# MD5 hash expected see above\n",
    "print()\n",
    "print('MD5 hash of the file is: ', md5_computed)\n",
    "\n",
    "# check with assertion\n",
    "assert (md5_computed == md5_cat)\n",
    "print(\"MD5 hash check OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving to a pandas dataframe\n",
    "df_orig = ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that will be used\n",
    "PREDICTOR = 'SeriousDlqin2yrs'\n",
    "\n",
    "unused_feat = ['id']\n",
    "\n",
    "num_col_list = ['RevolvingUtilizationOfUnsecuredLines','DebtRatio', 'MonthlyIncome']\n",
    "\n",
    "cat_col_list = ['age', 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfOpenCreditLinesAndLoans', \n",
    "                'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "               'NumberOfDependents']\n",
    "\n",
    "all_col_list = num_col_list + cat_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode cat features (with label encoder)\n",
    "nunique = data.nunique()\n",
    "types = data.dtypes\n",
    "\n",
    "categorical_columns = cat_col_list\n",
    "categorical_dims =  {}\n",
    "\n",
    "# I need to save the encoder list for the processing of the test set\n",
    "enc_list = []\n",
    " \n",
    "for col in cat_col_list:\n",
    "    print(col, data[col].nunique())\n",
    "    l_enc = LabelEncoder()\n",
    "    data[col] = l_enc.fit_transform(data[col].values)\n",
    "    \n",
    "    # save the encoder for the test set\n",
    "    enc_list.append(l_enc)\n",
    "    categorical_dims[col] = len(l_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train, validation\n",
    "FRAC = 0.8\n",
    "\n",
    "N_TRAIN = int(data.shape[0] * FRAC)\n",
    "N_VALID = data.shape[0] - N_TRAIN\n",
    "\n",
    "# before splitting, shuffle\n",
    "data = data.sample(frac = 1)\n",
    "\n",
    "df_train = data[:N_TRAIN]\n",
    "df_valid = data[N_TRAIN:]\n",
    "\n",
    "print('Number of records in train dataset:', N_TRAIN)\n",
    "print('Number of records in validation dataset:', N_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = df_train[PREDICTOR].values\n",
    "label_valid = df_valid[PREDICTOR].values\n",
    "\n",
    "df_train = df_train[all_col_list]\n",
    "df_valid = df_valid[all_col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ col for col in df_train.columns if col not in unused_feat+[PREDICTOR]] \n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Index of cat colums:', cat_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cardinality of cat columns:', cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLFLOW, configuration, for the callback\n",
    "\n",
    "MLF_TRACK_URI = 'http://130.61.20.111:5000'\n",
    "MLF_EXP_NAME = 'tabnet66gpu'\n",
    "MLF_RUN_NAME = 'tabnet66gpu-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters registered on MLFLOW\n",
    "params = {\n",
    "    \"epochs\" : 80,\n",
    "    \"batch_size\" : 2048,\n",
    "    \"n_steps\" : 1,\n",
    "    \"n_d\" : 36,\n",
    "    \"cat_emb_dim\" : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for MLFlow integration\n",
    "class MLCallback(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \n",
    "        mlflow.set_tracking_uri(MLF_TRACK_URI)\n",
    "        mlflow.set_experiment(MLF_EXP_NAME)\n",
    "        \n",
    "        print('Train begin...')\n",
    "        mlflow.start_run(run_name = MLF_RUN_NAME)\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        \n",
    "        mlflow.end_run()\n",
    "        print('Train end...')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # print(logs)\n",
    "        loss = logs[\"loss\"]\n",
    "        val_auc = logs[\"valid_auc\"]\n",
    "        train_auc = logs['train_auc']\n",
    "        \n",
    "        # send to MLFlow\n",
    "        mlflow.log_metric(\"train_auc\", train_auc)\n",
    "        mlflow.log_metric(\"valid_auc\", val_auc)\n",
    "        mlflow.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "EPOCHS = params['epochs']\n",
    "BATCH_SIZE = params['batch_size']\n",
    "N_STEPS = params['n_steps']\n",
    "CAT_EMB_DIM = params['cat_emb_dim']\n",
    "N_D = params['n_d']\n",
    "N_A = N_D\n",
    "\n",
    "# callback for MLFlow integration\n",
    "mlcbck = MLCallback()\n",
    "\n",
    "clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                       cat_dims=cat_dims,\n",
    "                       cat_emb_dim=CAT_EMB_DIM,\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=1e-2),\n",
    "                       scheduler_params={\"step_size\":30, # how to use learning rate scheduler\n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='sparsemax',\n",
    "                       n_steps = N_STEPS,\n",
    "                       n_d = N_D,\n",
    "                       n_a = N_A\n",
    "                      )\n",
    "\n",
    "clf.fit(df_train.values, label_train,\n",
    "        eval_set=[(df_train.values, label_train),(df_valid.values, label_valid)],\n",
    "        max_epochs = EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        patience = 15,\n",
    "        eval_name=['train', 'valid'],\n",
    "        eval_metric=['auc'],\n",
    "        callbacks = [mlcbck]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot auc\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('AUC')\n",
    "plt.plot(clf.history['train_auc'], label='Training AUC')\n",
    "plt.plot(clf.history['valid_auc'], label='Validation AUC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(True);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-bullet",
   "metadata": {},
   "source": [
    "### Model interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at feature importance\n",
    "# plt.bar(x= range(len(clf.feature_importances_)), height=clf.feature_importances_, );\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature importance')\n",
    "plt.bar(x= all_col_list, height=clf.feature_importances_, );\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-sigma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-external",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-rider",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlgpuv1]",
   "language": "python",
   "name": "conda-env-mlgpuv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
